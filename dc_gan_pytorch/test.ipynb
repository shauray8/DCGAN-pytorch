{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important stuff\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "\n",
    "# defining the discriminator\n",
    "\n",
    "class Discriminator_block(nn.Module):\n",
    "    def __init__(self, input_size, features):\n",
    "        super(Discriminator_block, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 64*64\n",
    "                \n",
    "            nn.Conv2d(input_size, features, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            # 32*32\n",
    "            \n",
    "            self.batchnorm(features, features*2, 4, 2, 1),\n",
    "            # 16*16\n",
    "            \n",
    "            self.batchnorm(features*2, features*4, 4, 2, 1),\n",
    "            # 8*8\n",
    "            \n",
    "            self.batchnorm(features*4, features*8, 4, 2, 1),\n",
    "            # 4*4\n",
    "\n",
    "            nn.Conv2d(features*8, 1, kernel_size=4, stride=2, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # Layers with batch normalization \n",
    "    def batchnorm(self, input, out, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                    nn.Conv2d(input,out,kernel_size,stride,padding,bias=False),\n",
    "                    nn.BatchNorm2d(out),\n",
    "                    nn.LeakyReLU(.2),\n",
    "                )\n",
    "\n",
    "    # forward function cause we have to have one \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generator for the GAN\n",
    " \n",
    "class Generator_block(nn.Module):\n",
    "    def __init__(self, noise,channels,features):\n",
    "        super(Generator_block, self).__init__()\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "                    self.batchnorm(noise, features*16, 4, 1, 0), #4*4\n",
    "                    self.batchnorm(features*16, features*8, 4, 2, 1), #8*8\n",
    "                    self.batchnorm(features*8, features*4, 4, 2, 1), #16*16\n",
    "                    self.batchnorm(features*4, features*2, 4, 2,1), #32*32\n",
    "                    nn.ConvTranspose2d(features*2, channels, kernel_size=4, stride=2, padding=1),\n",
    "                    nn.Tanh(),\n",
    "\n",
    "                )\n",
    "\n",
    "    # function for batch nomralization\n",
    "    def batchnorm(self, input, output, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                    nn.ConvTranspose2d(input, output, kernel_size, stride, padding, bias=False),\n",
    "                    nn.BatchNorm2d(output),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "    \n",
    "    # forward function \n",
    "    def forward(self, x):\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\acer\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torchvision\\transforms\\transforms.py:840: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n",
      "  warnings.warn(\"The use of the transforms.RandomSizedCrop transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    " \n",
    "import pickle\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "from discriminator import Discriminator_block\n",
    "from generator import Generator_block\n",
    "\n",
    "def initialize_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data,0.0,0.02)\n",
    "\n",
    "def image_show():\n",
    "    for _,(a,_) in enumerate(dataset_loader):\n",
    "        plt.imshow(a[0][0])\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5],\n",
    "                             std=[0.229])\n",
    "])\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "dataset = datasets.MNIST(root='../datatest//',transform=transform,download=True)\n",
    "dataset_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                             batch_size=batch_size, shuffle=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_epoch = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 2e-3\n",
    "NOISE = 100\n",
    "CHANNELS = 1\n",
    "FEATURES = 64\n",
    "\n",
    "\n",
    "G = Generator_block(NOISE, CHANNELS, FEATURES).to(device)\n",
    "D = Discriminator_block(CHANNELS, FEATURES).to(device)\n",
    "\n",
    "initialize_weight(G)\n",
    "initialize_weight(D)\n",
    "\n",
    "optim_G = optim.Adam(G.parameters(), lr = LEARNING_RATE, betas=(.5, 0.999))\n",
    "optim_D = optim.Adam(D.parameters(), lr = LEARNING_RATE, betas=(.5, 0.999))\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "D_labels = torch.ones([batch_size,1]).to(device)\n",
    "D_fakes = torch.zeros([batch_size,1]).to(device)\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "img_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Step: 934, D Loss: 2.5258987079723738e-05, G Loss: 11.486114501953125:   0%|                     | 0/1 [15:11<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in (wal := trange(max_epoch)):\n",
    "    for step,(images,_) in enumerate(dataset_loader):\n",
    "        x = images.to(device)\n",
    "        x_output = D(x).reshape(-1)\n",
    "        D_x_loss = loss_function(x_output, torch.ones_like(x_output).to(device))\n",
    "\n",
    "        z = torch.randn(batch_size, NOISE,1,1).to(device)\n",
    "        z_output = D(G(z)).reshape(-1)\n",
    "        D_z_loss = loss_function(z_output, torch.zeros_like(z_output).to(device))\n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "\n",
    "        optim_D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        z = torch.randn(batch_size, NOISE,1,1).to(device)\n",
    "        z_output = D(G(z)).reshape(-1)\n",
    "        G_loss = loss_function(z_output, torch.ones_like(z_output).to(device))\n",
    "\n",
    "        optim_G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "        wal.set_description(f'Step: {step}, D Loss: {D_loss.item()}, G Loss: {G_loss.item()}')\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = G(z).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4dbdf5fd1acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(img_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
